name: Continuous Deployment

on:
  push:
    branches: [ main ]
  workflow_run:
    workflows: ["Continuous Integration"]
    types:
      - completed
    branches: [main]

# These permissions are needed for OIDC authentication
permissions:
  id-token: write   # Required for OIDC
  contents: write   # Required for GitHub Releases
  security-events: write  # Required for CodeQL/Trivy SARIF upload

env:
  AWS_REGION: us-east-1
  EKS_CLUSTER_NAME: task-manager-eks
  ECR_BACKEND_REPOSITORY: task-manager-backend
  ECR_FRONTEND_REPOSITORY: task-manager-frontend

jobs:
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    environment:
      name: staging
      url: https://staging.task-manager.com
    
    permissions:
      id-token: write
      contents: read
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Configure AWS credentials via OIDC
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
        aws-region: ${{ env.AWS_REGION }}
        role-session-name: GitHubActions-Staging-Deploy
    
    - name: Get commit SHA
      id: vars
      run: echo "sha_short=$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT
    
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2
    
    - name: Verify EKS cluster exists
      run: |
        aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} || {
          echo "âŒ Cluster ${{ env.EKS_CLUSTER_NAME }} not found!"
          echo "Available clusters:"
          aws eks list-clusters --region ${{ env.AWS_REGION }} --output table
          exit 1
        }
    
    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}
    
    - name: Create namespace if not exists
      run: |
        kubectl create namespace task-manager-staging --dry-run=client -o yaml | kubectl apply -f -
    
    - name: Create or update secrets
      run: |
        # Delete existing secret if it exists
        kubectl delete secret task-manager-secrets -n task-manager-staging 2>/dev/null || true
        
        # Create fresh secret
        kubectl create secret generic task-manager-secrets \
          --namespace task-manager-staging \
          --from-literal=database-url="${{ secrets.STAGING_DATABASE_URL }}" \
          --from-literal=redis-url="${{ secrets.STAGING_REDIS_URL }}" \
          --from-literal=secret-key="${{ secrets.STAGING_SECRET_KEY }}"
    
    - name: Deploy backend to staging
      env:
        BACKEND_IMAGE: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_BACKEND_REPOSITORY }}:${{ steps.vars.outputs.sha_short }}
      run: |
        # Create or update backend deployment
        cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: task-manager-backend
  namespace: task-manager-staging
  labels:
    app: task-manager-backend
    environment: staging
spec:
  replicas: 2
  selector:
    matchLabels:
      app: task-manager-backend
  template:
    metadata:
      labels:
        app: task-manager-backend
        environment: staging
    spec:
      containers:
      - name: backend
        image: $BACKEND_IMAGE
        ports:
        - containerPort: 5000
          name: http
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: task-manager-secrets
              key: database-url
        - name: REDIS_URL
          valueFrom:
            secretKeyRef:
              name: task-manager-secrets
              key: redis-url
        - name: SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: task-manager-secrets
              key: secret-key
        - name: FLASK_ENV
          value: "staging"
        livenessProbe:
          httpGet:
            path: /health
            port: 5000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 5000
          initialDelaySeconds: 15
          periodSeconds: 5
          timeoutSeconds: 3
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
EOF
        
        # Wait for rollout
        kubectl rollout status deployment/task-manager-backend \
          -n task-manager-staging \
          --timeout=5m
    
    - name: Create backend service
      run: |
        cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: task-manager-backend
  namespace: task-manager-staging
  labels:
    app: task-manager-backend
spec:
  selector:
    app: task-manager-backend
  ports:
  - name: http
    port: 5000
    targetPort: 5000
  type: ClusterIP
EOF
    
    - name: Deploy frontend to staging
      env:
        FRONTEND_IMAGE: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_FRONTEND_REPOSITORY }}:${{ steps.vars.outputs.sha_short }}
      run: |
        # Create or update frontend deployment
        cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: task-manager-frontend
  namespace: task-manager-staging
  labels:
    app: task-manager-frontend
    environment: staging
spec:
  replicas: 2
  selector:
    matchLabels:
      app: task-manager-frontend
  template:
    metadata:
      labels:
        app: task-manager-frontend
        environment: staging
    spec:
      containers:
      - name: frontend
        image: $FRONTEND_IMAGE
        ports:
        - containerPort: 80
          name: http
        env:
        - name: REACT_APP_API_URL
          value: "http://task-manager-backend:5000/api"
        - name: REACT_APP_ENVIRONMENT
          value: "staging"
        livenessProbe:
          httpGet:
            path: /health
            port: 80
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 80
          initialDelaySeconds: 15
          periodSeconds: 5
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
EOF
        
        # Wait for rollout
        kubectl rollout status deployment/task-manager-frontend \
          -n task-manager-staging \
          --timeout=5m
    
    - name: Create frontend service
      run: |
        cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: task-manager-frontend
  namespace: task-manager-staging
  labels:
    app: task-manager-frontend
spec:
  selector:
    app: task-manager-frontend
  ports:
  - name: http
    port: 80
    targetPort: 80
  type: ClusterIP
EOF
    
    - name: Run database migrations
      run: |
        # Wait for backend pod to be ready
        kubectl wait --for=condition=ready pod -l app=task-manager-backend -n task-manager-staging --timeout=60s
        
        # Run migrations
        BACKEND_POD=$(kubectl get pod -n task-manager-staging -l app=task-manager-backend -o jsonpath='{.items[0].metadata.name}')
        kubectl exec -n task-manager-staging $BACKEND_POD -- flask db upgrade || echo "Migrations skipped or failed"
    
    - name: Run smoke tests
      run: |
        echo "ðŸ” Running smoke tests..."
        
        # Wait for services
        sleep 10
        
        # Test backend health via port-forward
        kubectl port-forward -n task-manager-staging svc/task-manager-backend 5000:5000 &
        PF_PID=$!
        sleep 5
        
        # Test health endpoint
        curl -f http://localhost:5000/health && echo "âœ… Backend health check passed" || echo "âš ï¸ Health check failed"
        
        # Kill port-forward
        kill $PF_PID 2>/dev/null || true
        
        echo "âœ… Smoke tests completed"
    
    - name: Deployment status
      if: always()
      run: |
        echo "## Staging Deployment Status" >> $GITHUB_STEP_SUMMARY
        echo "- **Backend image:** ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_BACKEND_REPOSITORY }}:${{ steps.vars.outputs.sha_short }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Frontend image:** ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_FRONTEND_REPOSITORY }}:${{ steps.vars.outputs.sha_short }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Deployment time:** $(date)" >> $GITHUB_STEP_SUMMARY
        echo "- **Status:** âœ… Success" >> $GITHUB_STEP_SUMMARY
    
    - name: Notify Slack on success
      if: success() && secrets.SLACK_WEBHOOK
      uses: rtCamp/action-slack-notify@v2
      env:
        SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
        SLACK_CHANNEL: deployments
        SLACK_COLOR: good
        SLACK_TITLE: "âœ… Staging Deployment Successful"
        SLACK_MESSAGE: "Task Manager staging updated with commit ${{ steps.vars.outputs.sha_short }}"
        SLACK_FOOTER: ""
    
    - name: Notify Slack on failure
      if: failure() && secrets.SLACK_WEBHOOK
      uses: rtCamp/action-slack-notify@v2
      env:
        SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
        SLACK_CHANNEL: deployments
        SLACK_COLOR: danger
        SLACK_TITLE: "âŒ Staging Deployment Failed"
        SLACK_MESSAGE: "Deployment failed for commit ${{ steps.vars.outputs.sha_short }}"
        SLACK_FOOTER: ""

  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [deploy-staging]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    environment:
      name: production
      url: https://app.task-manager.com
    
    permissions:
      id-token: write
      contents: write  # Needed for creating GitHub Release
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Configure AWS credentials via OIDC
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
        aws-region: ${{ env.AWS_REGION }}
        role-session-name: GitHubActions-Production-Deploy
    
    - name: Get commit SHA
      id: vars
      run: echo "sha_short=$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT
    
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2
    
    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}
    
    - name: Create namespace if not exists
      run: |
        kubectl create namespace task-manager-prod --dry-run=client -o yaml | kubectl apply -f -
    
    - name: Create or update secrets
      run: |
        # Delete existing secret if it exists
        kubectl delete secret task-manager-secrets -n task-manager-prod 2>/dev/null || true
        
        # Create fresh secret
        kubectl create secret generic task-manager-secrets \
          --namespace task-manager-prod \
          --from-literal=database-url="${{ secrets.PROD_DATABASE_URL }}" \
          --from-literal=redis-url="${{ secrets.PROD_REDIS_URL }}" \
          --from-literal=secret-key="${{ secrets.PROD_SECRET_KEY }}"
    
    - name: Deploy backend to production
      env:
        BACKEND_IMAGE: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_BACKEND_REPOSITORY }}:${{ steps.vars.outputs.sha_short }}
      run: |
        cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: task-manager-backend
  namespace: task-manager-prod
  labels:
    app: task-manager-backend
    environment: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: task-manager-backend
  template:
    metadata:
      labels:
        app: task-manager-backend
        environment: production
    spec:
      containers:
      - name: backend
        image: $BACKEND_IMAGE
        ports:
        - containerPort: 5000
          name: http
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: task-manager-secrets
              key: database-url
        - name: REDIS_URL
          valueFrom:
            secretKeyRef:
              name: task-manager-secrets
              key: redis-url
        - name: SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: task-manager-secrets
              key: secret-key
        - name: FLASK_ENV
          value: "production"
        livenessProbe:
          httpGet:
            path: /health
            port: 5000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 5000
          initialDelaySeconds: 15
          periodSeconds: 5
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
EOF
        
        kubectl rollout status deployment/task-manager-backend -n task-manager-prod --timeout=5m
    
    - name: Create backend service
      run: |
        cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: task-manager-backend
  namespace: task-manager-prod
  labels:
    app: task-manager-backend
spec:
  selector:
    app: task-manager-backend
  ports:
  - name: http
    port: 5000
    targetPort: 5000
  type: ClusterIP
EOF
    
    - name: Deploy frontend to production
      env:
        FRONTEND_IMAGE: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_FRONTEND_REPOSITORY }}:${{ steps.vars.outputs.sha_short }}
      run: |
        cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: task-manager-frontend
  namespace: task-manager-prod
  labels:
    app: task-manager-frontend
    environment: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: task-manager-frontend
  template:
    metadata:
      labels:
        app: task-manager-frontend
        environment: production
    spec:
      containers:
      - name: frontend
        image: $FRONTEND_IMAGE
        ports:
        - containerPort: 80
          name: http
        env:
        - name: REACT_APP_API_URL
          value: "http://task-manager-backend:5000/api"
        - name: REACT_APP_ENVIRONMENT
          value: "production"
        livenessProbe:
          httpGet:
            path: /health
            port: 80
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 80
          initialDelaySeconds: 15
          periodSeconds: 5
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
EOF
        
        kubectl rollout status deployment/task-manager-frontend -n task-manager-prod --timeout=5m
    
    - name: Create frontend service
      run: |
        cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: task-manager-frontend
  namespace: task-manager-prod
  labels:
    app: task-manager-frontend
spec:
  selector:
    app: task-manager-frontend
  ports:
  - name: http
    port: 80
    targetPort: 80
  type: ClusterIP
EOF
    
    - name: Create ingress
      run: |
        cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: task-manager-ingress
  namespace: task-manager-prod
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}]'
    alb.ingress.kubernetes.io/healthcheck-path: /health
spec:
  rules:
  - host: app.task-manager.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: task-manager-frontend
            port:
              number: 80
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: task-manager-backend
            port:
              number: 5000
EOF
    
    - name: Run database migrations
      run: |
        kubectl wait --for=condition=ready pod -l app=task-manager-backend -n task-manager-prod --timeout=60s
        BACKEND_POD=$(kubectl get pod -n task-manager-prod -l app=task-manager-backend -o jsonpath='{.items[0].metadata.name}')
        kubectl exec -n task-manager-prod $BACKEND_POD -- flask db upgrade || echo "Migrations skipped or failed"
    
    - name: Get application URL
      run: |
        sleep 30
        ALB_URL=$(kubectl get ingress -n task-manager-prod -o jsonpath='{.items[0].status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "pending")
        echo "APPLICATION_URL=http://$ALB_URL" >> $GITHUB_ENV
        echo "ðŸŒ Application URL: http://$ALB_URL" >> $GITHUB_STEP_SUMMARY
    
    - name: Create GitHub Release
      uses: softprops/action-gh-release@v2
      with:
        tag_name: v${{ github.run_number }}
        name: Release v${{ github.run_number }}
        body: |
          ## Production Deployment v${{ github.run_number }}
          
          ### Images Deployed
          - **Backend:** ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_BACKEND_REPOSITORY }}:${{ steps.vars.outputs.sha_short }}
          - **Frontend:** ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_FRONTEND_REPOSITORY }}:${{ steps.vars.outputs.sha_short }}
          
          ### Commit
          - **SHA:** ${{ github.sha }}
          - **Message:** ${{ github.event.head_commit.message }}
          
          ### Deployment Time
          - $(date)
        draft: false
        prerelease: false
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Notify Slack
      if: secrets.SLACK_WEBHOOK
      uses: rtCamp/action-slack-notify@v2
      env:
        SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
        SLACK_CHANNEL: deployments
        SLACK_COLOR: ${{ job.status == 'success' && 'good' || 'danger' }}
        SLACK_TITLE: "${{ job.status == 'success' && 'âœ…' || 'âŒ' }} Production Deployment ${{ job.status }}"
        SLACK_MESSAGE: "Task Manager production updated to version v${{ github.run_number }}"
        SLACK_FOOTER: ""

  rollback:
    name: Automatic Rollback
    runs-on: ubuntu-latest
    if: failure()
    needs: [deploy-production]
    
    permissions:
      id-token: write
      contents: read
    
    steps:
    - name: Configure AWS credentials via OIDC
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}
        aws-region: ${{ env.AWS_REGION }}
        role-session-name: GitHubActions-Rollback
    
    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}
    
    - name: Rollback to previous version
      run: |
        echo "ðŸ”„ Rolling back production deployment..."
        
        # Rollback backend
        kubectl rollout undo deployment/task-manager-backend -n task-manager-prod || true
        
        # Rollback frontend
        kubectl rollout undo deployment/task-manager-frontend -n task-manager-prod || true
        
        # Wait for rollback
        kubectl rollout status deployment/task-manager-backend -n task-manager-prod --timeout=3m || true
        kubectl rollout status deployment/task-manager-frontend -n task-manager-prod --timeout=3m || true
        
        echo "âœ… Rollback completed" >> $GITHUB_STEP_SUMMARY
    
    - name: Notify rollback
      if: secrets.SLACK_WEBHOOK
      uses: rtCamp/action-slack-notify@v2
      env:
        SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
        SLACK_CHANNEL: deployments
        SLACK_COLOR: warning
        SLACK_TITLE: "ðŸ”„ Production Rollback Initiated"
        SLACK_MESSAGE: "Production deployment failed - rolled back to previous version"
        SLACK_FOOTER: ""